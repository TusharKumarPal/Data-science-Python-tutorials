{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Display plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Digits Sample Data\n",
    "\n",
    "We'll be using the same dataset we used for the [classification notebook](classification.ipynb) - a dataset of 8x8 pixel handwritten digits such as the following:\n",
    "\n",
    "![digits](digits.png)\n",
    "\n",
    "The data is in the form of a 64 element array of integers representing grayscale values for each pixel.  Therefore the data is 64-dimensional.  Often it is useful to be able to reduce the number of dimensions before working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load all the samples for all digits 0-9\n",
    "digits = load_digits()\n",
    "\n",
    "# Assign the matrices to a variable `data`\n",
    "data = digits.data\n",
    "\n",
    "# Assign the labels to a variable `target`\n",
    "target = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is a technique used to find vectors, called principal components, in your data along which there is the greatest amount of variance.  Essentially finding the most meaningful differences in the data.  There will be as many principal components as there are dimensions.  But they will be ordered by significance and by only using the first n principal components, the dimensionality of the data can possibly be reduced without losing much information.\n",
    "\n",
    "scikit-learn gives us the ability to find the principal components of a dataset and then project the data onto those components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the PCA model\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the model to our data and then project the data onto the components\n",
    "Xproj = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get a better sense of what happened, we can visualize just the first two dimensions of the projected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# Scatter the projections of the first and second principal components and color them by their labels\n",
    "s = ax.scatter(Xproj[:, 0], Xproj[:, 1], c=target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('gist_rainbow', 10))\n",
    "\n",
    "# Create a colorbar for reference\n",
    "figure.colorbar(s, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visually, we can can see fairly distinct clusters of colors.  What this means is that by reducing our 64-dimension feature space down to just 2 dimensions we can still retain a lot of the information.\n",
    "\n",
    "We can see this another way by seeing how few of the principal components we need to use to distinguish the digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "figure, (axs) = plt.subplots(figsize=(16,1), ncols=16)\n",
    "for i,ax in enumerate(axs):\n",
    "    # Restore the first i components of the reduced data\n",
    "    # This involves multiplying the projection by the principal components,\n",
    "    # adding up the values and then adding the mean for each pixel.\n",
    "    digit = np.sum(pca.components_[:i] * Xproj[0][:i][:,np.newaxis], axis=0) + pca.mean_\n",
    "    \n",
    "    # Show an \"image\" - a nxn array of values\n",
    "    ax.imshow(digit.astype(int).reshape((8,8)), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above shows a '0' restored using n principal components from n=0 to n=7.  By 2 or 3 principal components the 0 is unmistakebale and by 5 it is getting almost no new information by adding more components.  Therefore we gain very little for using more than the first few components and can work with much fewer than 64 dimensions.\n",
    "\n",
    "This is also an example of how dimensionality reduction can be used in data compression.\n",
    "\n",
    "scikit-learn offers a few types of PCA: Kernel PCA, Sparse PCA, Randomized PCA.  For example, using [Kernel PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Create the Kernel PCA model with a 4th degree polynomial kernel, gamma of .009 and coef0 of 120\n",
    "pca = KernelPCA(kernel='poly', degree=4, gamma=.009, coef0=120)\n",
    "\n",
    "# Fit the model to our data and then project the data onto the components\n",
    "Xproj = pca.fit_transform(data)\n",
    "\n",
    "# Create a figure\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# Scatter the projections of the first and second principal components and color them by their labels\n",
    "s = ax.scatter(Xproj[:, 0], Xproj[:, 1], c=target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('gist_rainbow', 10))\n",
    "\n",
    "# Create a colorbar for reference\n",
    "figure.colorbar(s, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PCA + Classification\n",
    "\n",
    "Dimensionality reduction often might be used before running the classification algorithms discussed in the [classification notebook](classification.ipynb) due to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).  For example, let's retry using decision trees with just 8 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Use the first 8 dimensions of the reduced data to create a train/test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    Xproj[:,:8], target, test_size=.25, random_state=0\n",
    ")\n",
    "\n",
    "# Create the model as we did before\n",
    "model = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "# Fit it to our reduced data\n",
    "model.fit(data_train, target_train)\n",
    "\n",
    "# Use the model to predict labels for our training set\n",
    "pred_train = model.predict(data_train)\n",
    "\n",
    "# And for the test set\n",
    "pred_test = model.predict(data_test)\n",
    "\n",
    "# Print the accuracy for the training set\n",
    "print(\"Training Accuracy:\", accuracy_score(target_train, pred_train))\n",
    "\n",
    "# Print the accuracy for the test set\n",
    "print(\"Testing Accuracy:\", accuracy_score(target_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For comparison, here is the same thing done on the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use the original to create a train/test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, test_size=.25, random_state=0\n",
    ")\n",
    "\n",
    "# Create the model as we did before\n",
    "model = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "# Fit it to our reduced data\n",
    "model.fit(data_train, target_train)\n",
    "\n",
    "# Use the model to predict labels for our training set\n",
    "pred_train = model.predict(data_train)\n",
    "\n",
    "# And for the test set\n",
    "pred_test = model.predict(data_test)\n",
    "\n",
    "# Print the accuracy for the training set\n",
    "print(\"Training Accuracy:\", accuracy_score(target_train, pred_train))\n",
    "\n",
    "# Print the accuracy for the test set\n",
    "print(\"Testing Accuracy:\", accuracy_score(target_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, the model performs as well or slightly better (less overfitting).\n",
    "\n",
    "# Further Reading\n",
    "* [Decomposing signals in components](http://scikit-learn.org/stable/modules/decomposition.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
